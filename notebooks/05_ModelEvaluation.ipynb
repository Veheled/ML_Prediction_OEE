{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mount Correct Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd #INSERT FOLDER PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239158514
        }
      },
      "outputs": [],
      "source": [
        "from modules.model_training import Model_Trainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup & Run Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239158613
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "feature_columns = ['ProductCode', 'Code', 'Previous_ProductCode', 'OrderQuantity'] ### ORDER INFOS\n",
        "feature_columns += ['FS_Breite', 'FS_Länge', 'FS_Tiefe', 'PBL_Breite', 'PBL_Länge', 'Tuben_Durchmesser', 'CALC_PACKGROESSE', 'Tuben_Länge', 'CALC_WIRKSTOFF', 'CALC_ALUFOLIE'] # PRODUCT INFOS\n",
        "feature_columns += ['10th_Percentile_Auftragswechsel', '10th_Percentile_Primär', '10th_Percentile_Sekundär'] # HISTORIC PRODUCT CHANGE INFOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239159392
        }
      },
      "outputs": [],
      "source": [
        "model_trainer_1 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.3,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_2 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.25,\n",
        "    scaling_enabled=True,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_3 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.2,\n",
        "    scaling_enabled=True,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_4 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.15,\n",
        "    product_encoding_method='ordinal',\n",
        "    scaling_enabled=True,\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_5 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=True,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_6 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.05,\n",
        "    scaling_enabled=True,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='BASE'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run Validation Ratio Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239175951
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "results_1 = model_trainer_1.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_2 = model_trainer_2.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_3 = model_trainer_3.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_4 = model_trainer_4.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_5 = model_trainer_5.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_6 = model_trainer_6.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_error_summary_by_experiment(result_dicts):\n",
        "    # Initialize a list to hold the aggregated data per experiment\n",
        "    summary_rows = []\n",
        "    \n",
        "    # Loop through each experiment (result dictionary)\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        # Create a dictionary to store RMSEs by target type\n",
        "        target_rmse = {}\n",
        "        \n",
        "        for result in result_model_performance:\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Collect val_RMSE for each target\n",
        "            if target not in target_rmse:\n",
        "                target_rmse[target] = []\n",
        "            target_rmse[target].append(val_rmse)\n",
        "        \n",
        "        # For each target, calculate min, mean, max across all model types\n",
        "        summary_row = {'experiment': f'Experiment {i}'}\n",
        "        for target, rmses in target_rmse.items():\n",
        "            summary_row[f'{target}_min'] = min(rmses)\n",
        "            summary_row[f'{target}_mean'] = sum(rmses) / len(rmses)\n",
        "            summary_row[f'{target}_max'] = max(rmses)\n",
        "        \n",
        "        # Append the row to summary\n",
        "        summary_rows.append(summary_row)\n",
        "    \n",
        "    # Convert the summary into a pandas DataFrame\n",
        "    df_summary = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return df_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)  # Display all rows\n",
        "pd.set_option('display.max_columns', None)  # Display all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have results_1 to results_6\n",
        "result_dicts = [results_1, results_2, results_3, results_4, results_5, results_6]\n",
        "df_summary_val_ratio_test = extract_error_summary_by_experiment(result_dicts)\n",
        "display(df_summary_val_ratio_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling Test Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_7 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='SCALING_TEST'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_8 = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=True,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='SCALING_TEST'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_7 = model_trainer_7.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_8 = model_trainer_8.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_error_stats(result_dicts):\n",
        "    rows = []\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        for result in result_model_performance:\n",
        "            model_type = result['type']\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Check if the model type and target combination is already in the list\n",
        "            row = next((r for r in rows if r['type'] == model_type and r['target'] == target), None)\n",
        "            \n",
        "            if row:\n",
        "                # Update the list of val_RMSE for the specific model type and target\n",
        "                row['val_RMSEs'].append(val_rmse)\n",
        "            else:\n",
        "                # Append new entry with the val_RMSE initialized in a list\n",
        "                rows.append({\n",
        "                    'type': model_type,\n",
        "                    'target': target,\n",
        "                    'val_RMSEs': [val_rmse]\n",
        "                })\n",
        "    \n",
        "    ## Calculate min, mean, max for each entry and prepare the final table\n",
        "    summary_rows = []\n",
        "    for row in rows:\n",
        "        diff_rmse = row['val_RMSEs'][1]-row['val_RMSEs'][0]\n",
        "        \n",
        "        summary_rows.append({\n",
        "            'type': row['type'],\n",
        "            'target': row['target'],\n",
        "            'scaling_off': row['val_RMSEs'][0],\n",
        "            'scaling_on': row['val_RMSEs'][1],\n",
        "            'diff_rmse': diff_rmse\n",
        "        })\n",
        "    \n",
        "    # Convert to pandas DataFrame for better presentation\n",
        "    df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_rows(result_dicts):\n",
        "    rows = []\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        for result in result_model_performance:\n",
        "            model_type = result['type']\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Check if the model type and target combination is already in the list\n",
        "            row = next((r for r in rows if r['type'] == model_type and r['target'] == target and r['experiment'] ==  f'Experiment {i}'), None)\n",
        "            \n",
        "            if row:\n",
        "                # Update the list of val_RMSE for the specific model type and target\n",
        "                row['val_RMSEs'].append(val_rmse)\n",
        "            else:\n",
        "                # Append new entry with the val_RMSE initialized in a list\n",
        "                rows.append({\n",
        "                    'experiment': f'Experiment {i}',\n",
        "                    'type': model_type,\n",
        "                    'target': target,\n",
        "                    'val_RMSEs': [val_rmse]\n",
        "                })\n",
        "    \n",
        "    # Convert to pandas DataFrame for better presentation\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dicts = [results_7, results_8]\n",
        "df_summary_scaling_test = extract_rows(result_dicts)\n",
        "display(df_summary_scaling_test[df_summary_scaling_test['target']=='OEE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary_scaling_test = extract_error_stats(result_dicts)\n",
        "display(df_summary_scaling_test[df_summary_scaling_test['target']=='OEE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame to only include specific targets\n",
        "df_filtered = df_summary_scaling_test[df_summary_scaling_test['target'].isin(['OEE', 'PERF', 'AVAIL', 'QUAL'])]\n",
        "\n",
        "# Group by 'type' and calculate the mean of 'scaling_off' and 'scaling_on'\n",
        "df_grouped = df_filtered.groupby('type').agg({\n",
        "    'scaling_off': 'mean',\n",
        "    'scaling_on': 'mean'\n",
        "})\n",
        "\n",
        "# Calculate the difference between 'scaling_on' and 'scaling_off'\n",
        "df_grouped['diff_rmse'] = df_grouped['scaling_on'] - df_grouped['scaling_off']\n",
        "\n",
        "# Optionally, print the result\n",
        "display(df_grouped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoding Test Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_ordinal = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='ENCODING_TEST'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_label = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='label',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='ENCODING_TEST'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_ordinal = model_trainer_ordinal.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_label = model_trainer_label.run_training_pipeline(verbose_train=0, verbose_test=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_error_stats_enc(result_dicts):\n",
        "    rows = []\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        for result in result_model_performance:\n",
        "            model_type = result['type']\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Check if the model type and target combination is already in the list\n",
        "            row = next((r for r in rows if r['type'] == model_type and r['target'] == target), None)\n",
        "            \n",
        "            if row:\n",
        "                # Update the list of val_RMSE for the specific model type and target\n",
        "                row['val_RMSEs'].append(val_rmse)\n",
        "            else:\n",
        "                # Append new entry with the val_RMSE initialized in a list\n",
        "                rows.append({\n",
        "                    'type': model_type,\n",
        "                    'target': target,\n",
        "                    'val_RMSEs': [val_rmse]\n",
        "                })\n",
        "    \n",
        "    ## Calculate min, mean, max for each entry and prepare the final table\n",
        "    summary_rows = []\n",
        "    for row in rows:\n",
        "        diff_rmse = row['val_RMSEs'][1]-row['val_RMSEs'][0]\n",
        "        \n",
        "        summary_rows.append({\n",
        "            'type': row['type'],\n",
        "            'target': row['target'],\n",
        "            'ordinal': row['val_RMSEs'][0],\n",
        "            'label': row['val_RMSEs'][1],\n",
        "            'diff_rmse': diff_rmse\n",
        "        })\n",
        "    \n",
        "    # Convert to pandas DataFrame for better presentation\n",
        "    df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_dicts = [results_ordinal, results_label]\n",
        "df_summary_scaling_test = extract_error_stats_enc(result_dicts)\n",
        "display(df_summary_scaling_test[df_summary_scaling_test['target']=='OEE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame to only include specific targets\n",
        "df_filtered = df_summary_scaling_test[df_summary_scaling_test['target'].isin(['OEE', 'PERF', 'AVAIL', 'QUAL'])]\n",
        "\n",
        "# Group by 'type' and calculate the mean of 'scaling_off' and 'scaling_on'\n",
        "df_grouped = df_filtered.groupby('type').agg({\n",
        "    'ordinal': 'mean',\n",
        "    'label': 'mean'\n",
        "})\n",
        "\n",
        "# Calculate the difference between 'scaling_on' and 'scaling_off'\n",
        "df_grouped['diff_rmse'] = df_grouped['ordinal'] - df_grouped['label']\n",
        "\n",
        "# Optionally, print the result\n",
        "print(df_grouped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scienceplots\n",
        "import pandas as pd\n",
        "\n",
        "# Use the 'science' style for plots\n",
        "plt.style.use('science')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a DataFrame\n",
        "model_effiency_analysis = pd.DataFrame(results)\n",
        "\n",
        "# Plotting function\n",
        "def plot_model_performance(dataframe, error_function):\n",
        "    # Get unique targets\n",
        "    targets = dataframe['target'].unique()\n",
        "    best_models = []\n",
        "\n",
        "    for target in targets:\n",
        "        # Filter data for the current target\n",
        "        target_data = dataframe[dataframe['target'] == target]\n",
        "        target_data = target_data.sort_values(by='type')\n",
        "\n",
        "        # Find the minimum error value\n",
        "        min_error = target_data[error_function].min()\n",
        "        min_model = target_data[target_data[error_function]==min_error]\n",
        "\n",
        "        best_models.append(\n",
        "            {\n",
        "                \"target\": target,\n",
        "                \"model\": min_model['model'],\n",
        "                \"name\": min_model['type'].values[0],\n",
        "                \"error\": min_error,\n",
        "                \"error_name\": error_function,\n",
        "                \"X_val\": min_model['X_val']\n",
        "            }\n",
        "        )\n",
        "        # Generate colors based on the RMSE value\n",
        "        colors = ['green' if rmse == min_error else 'skyblue' for rmse in target_data[error_function]]\n",
        "\n",
        "        # Create a bar plot for the current target\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Update the plt.bar line to use the colors list\n",
        "        plt.bar(target_data['type'], target_data[error_function], color=colors)\n",
        " \n",
        "        # Adding title and labels\n",
        "        plt.title(f'Model Performance for Target: {target}')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel(error_function)\n",
        "        plt.xticks(rotation=90)  # for vertical labels\n",
        " \n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "    return best_models\n",
        "\n",
        "# Call the function to plot the performance of models\n",
        "model_eval = plot_model_performance(model_effiency_analysis, 'val_RMSE')\n",
        "#print(\"~~~~~~~~~~~~~~~~~~~~~~~\\nMEAN AVERAGE ERROR\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "#model_eval = plot_model_performance(model_effiency_analysis, 'MAE')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
