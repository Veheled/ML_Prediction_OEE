{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mount Correct Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd #INSERT FOLDER PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239158514
        }
      },
      "outputs": [],
      "source": [
        "from modules.model_training import Model_Trainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup & Run Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724239158613
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "feature_columns = ['ProductCode', 'Code', 'Previous_ProductCode', 'OrderQuantity'] ### ORDER INFOS\n",
        "feature_columns += ['FS_Breite', 'FS_Länge', 'FS_Tiefe', 'PBL_Breite', 'PBL_Länge', 'Tuben_Durchmesser', 'CALC_PACKGROESSE', 'Tuben_Länge', 'CALC_WIRKSTOFF', 'CALC_ALUFOLIE'] # PRODUCT INFOS\n",
        "feature_columns += ['10th_Percentile_Auftragswechsel', '10th_Percentile_Primär', '10th_Percentile_Sekundär'] # HISTORIC PRODUCT CHANGE INFOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_error_summary_by_experiment(result_dicts):\n",
        "    # Initialize a list to hold the aggregated data per experiment\n",
        "    summary_rows = []\n",
        "    \n",
        "    # Loop through each experiment (result dictionary)\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        # Create a dictionary to store RMSEs by target type\n",
        "        target_rmse = {}\n",
        "        \n",
        "        for result in result_model_performance:\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Collect val_RMSE for each target\n",
        "            if target not in target_rmse:\n",
        "                target_rmse[target] = []\n",
        "            target_rmse[target].append(val_rmse)\n",
        "        \n",
        "        # For each target, calculate min, mean, max across all model types\n",
        "        summary_row = {'experiment': f'Experiment {i}'}\n",
        "        for target, rmses in target_rmse.items():\n",
        "            summary_row[f'{target}_min'] = min(rmses)\n",
        "            summary_row[f'{target}_mean'] = sum(rmses) / len(rmses)\n",
        "            summary_row[f'{target}_max'] = max(rmses)\n",
        "        \n",
        "        # Append the row to summary\n",
        "        summary_rows.append(summary_row)\n",
        "    \n",
        "    # Convert the summary into a pandas DataFrame\n",
        "    df_summary = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return df_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)  # Display all rows\n",
        "pd.set_option('display.max_columns', None)  # Display all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_error_stats(result_dicts):\n",
        "    rows = []\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        for result in result_model_performance:\n",
        "            model_type = result['type']\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Check if the model type and target combination is already in the list\n",
        "            row = next((r for r in rows if r['type'] == model_type and r['target'] == target), None)\n",
        "            \n",
        "            if row:\n",
        "                # Update the list of val_RMSE for the specific model type and target\n",
        "                row['val_RMSEs'].append(val_rmse)\n",
        "            else:\n",
        "                # Append new entry with the val_RMSE initialized in a list\n",
        "                rows.append({\n",
        "                    'type': model_type,\n",
        "                    'target': target,\n",
        "                    'val_RMSEs': [val_rmse]\n",
        "                })\n",
        "    \n",
        "    ## Calculate min, mean, max for each entry and prepare the final table\n",
        "    summary_rows = []\n",
        "    for row in rows:\n",
        "        diff_rmse = row['val_RMSEs'][1]-row['val_RMSEs'][0]\n",
        "        \n",
        "        summary_rows.append({\n",
        "            'type': row['type'],\n",
        "            'target': row['target'],\n",
        "            'scaling_off': row['val_RMSEs'][0],\n",
        "            'scaling_on': row['val_RMSEs'][1],\n",
        "            'diff_rmse': diff_rmse\n",
        "        })\n",
        "    \n",
        "    # Convert to pandas DataFrame for better presentation\n",
        "    df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_rows(result_dicts):\n",
        "    rows = []\n",
        "    for i, result_model_performance in enumerate(result_dicts, 1):\n",
        "        for result in result_model_performance:\n",
        "            model_type = result['type']\n",
        "            target = result['target']\n",
        "            val_rmse = result['val_RMSE']\n",
        "            \n",
        "            # Check if the model type and target combination is already in the list\n",
        "            row = next((r for r in rows if r['type'] == model_type and r['target'] == target and r['experiment'] ==  f'Experiment {i}'), None)\n",
        "            \n",
        "            if row:\n",
        "                # Update the list of val_RMSE for the specific model type and target\n",
        "                row['val_RMSEs'].append(val_rmse)\n",
        "            else:\n",
        "                # Append new entry with the val_RMSE initialized in a list\n",
        "                rows.append({\n",
        "                    'experiment': f'Experiment {i}',\n",
        "                    'type': model_type,\n",
        "                    'target': target,\n",
        "                    'val_RMSEs': [val_rmse]\n",
        "                })\n",
        "    \n",
        "    # Convert to pandas DataFrame for better presentation\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_no_hpo = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'svr', 'xgb', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=False,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='HYPERPARAMETER_OPTIMIZATION'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_trainer_hpo = Model_Trainer(\n",
        "    raw_data_folder_path = '00_RawData/',\n",
        "    integrated_data_path = '01_IntegratedData/',\n",
        "    feature_data_path= '02_FeatureData/',\n",
        "    preprocessed_data_folder_path='03_Preprocessed_FeatureData/',\n",
        "    frontend_reference_folder_path='04_Frontend_ReferenceData/',\n",
        "    feature_list=feature_columns,\n",
        "    model_targets=['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT'], # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "    models_to_train=['linear', 'ridge', 'poly', 'dt', 'rf', 'svr', 'xgb', 'catboost', 'lgbm', 'NN'], # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "    validation_ratio=0.1,\n",
        "    scaling_enabled=False,\n",
        "    product_encoding_method='ordinal',\n",
        "    save_models=False,\n",
        "    model_optimization_do=True,\n",
        "    optimization_mode='optunasearch',\n",
        "    model_test_name='HYPERPARAMETER_OPTIMIZATION'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_no_hpo = model_trainer_no_hpo.run_training_pipeline(verbose_train=0, verbose_test=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_hpo = model_trainer_hpo.run_training_pipeline(verbose_train=0, verbose_test=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Helper function to calculate RMSE, MAE, and R2\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "# Function to revalidate models and generate the table for multiple targets\n",
        "def revalidate_models_all_targets(result_dict_no_opt=None, result_dict_opt=None):\n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    all_results = []\n",
        "\n",
        "    if result_dict_opt is None:\n",
        "        # If only one dictionary is provided, calculate metrics just for it\n",
        "        for model_entry in result_dict_no_opt:\n",
        "            model_type = model_entry['type']\n",
        "            target = model_entry['target']  # Assuming the 'target' is a string or label for the model's target\n",
        "            \n",
        "            # Calculate metrics for the single dictionary\n",
        "            y_true = model_entry['y_val']  # True values\n",
        "            y_pred = model_entry['y_pred']  # Predicted values\n",
        "            rmse, mae, r2 = calculate_metrics(y_true, y_pred)\n",
        "\n",
        "            # Collect results for the single dictionary\n",
        "            all_results.append([target, model_type, 'RMSE', rmse, None, None])\n",
        "            all_results.append([target, model_type, 'MAE', mae, None, None])\n",
        "            all_results.append([target, model_type, 'R2', r2, None, None])\n",
        "    else:\n",
        "        # Loop through models in both non-optimized and optimized dictionaries\n",
        "        for model_entry_no_opt, model_entry_opt in zip(result_dict_no_opt, result_dict_opt):\n",
        "            model_type = model_entry_no_opt['type']\n",
        "            target = model_entry_no_opt['target']  # Assuming 'target' is a label or string for each target in the model\n",
        "            \n",
        "            # Calculate metrics for non-optimized model\n",
        "            y_true_no_opt = model_entry_no_opt['y_val']  # True values for non-optimized model\n",
        "            y_pred_no_opt = model_entry_no_opt['y_pred']  # Predicted values for non-optimized model\n",
        "            rmse_no_opt, mae_no_opt, r2_no_opt = calculate_metrics(y_true_no_opt, y_pred_no_opt)\n",
        "\n",
        "            # Calculate metrics for optimized model\n",
        "            y_true_opt = model_entry_opt['y_val']  # True values for optimized model\n",
        "            y_pred_opt = model_entry_opt['y_pred']  # Predicted values for optimized model\n",
        "            rmse_opt, mae_opt, r2_opt = calculate_metrics(y_true_opt, y_pred_opt)\n",
        "\n",
        "            # Calculate differences between optimized and non-optimized models\n",
        "            rmse_diff = rmse_opt - rmse_no_opt\n",
        "            mae_diff = mae_opt - mae_no_opt\n",
        "            r2_diff = r2_opt - r2_no_opt\n",
        "\n",
        "            # Collect results for non-optimized, optimized, and the difference\n",
        "            all_results.append([target, model_type, 'RMSE', rmse_no_opt, rmse_opt, rmse_diff])\n",
        "            all_results.append([target, model_type, 'MAE', mae_no_opt, mae_opt, mae_diff])\n",
        "            all_results.append([target, model_type, 'R2', r2_no_opt, r2_opt, r2_diff])\n",
        "\n",
        "    # Create DataFrame to present the results\n",
        "    df_results = pd.DataFrame(all_results, columns=['Target', 'Model Type', 'Metric', 'Non-Optimized', 'Optimized', 'Difference'])\n",
        "    \n",
        "    # Pivot table to present it in the required format, with \"Target\" as the first column\n",
        "    df_pivot = df_results.pivot_table(index=['Target', 'Model Type', 'Metric'], values=['Non-Optimized', 'Optimized', 'Difference'], aggfunc='first')\n",
        "    \n",
        "    return df_pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# Assuming result_dict_no_opt and result_dict_opt are your two dictionaries\n",
        "df_pivot = revalidate_models_all_targets(results_no_hpo, results_hpo)\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Composite Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Helper function to calculate RMSE, MAE, and R2\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "# Function to calculate composite OEE metrics\n",
        "def calculate_composite_metrics(result_dict_no_opt=None, result_dict_opt=None):\n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    all_results = []\n",
        "\n",
        "    if result_dict_opt is None:\n",
        "        # Only non-optimized models provided\n",
        "        # Build a mapping from 'type' to a dict of 'target' to model entries\n",
        "        type_to_target_to_entry = {}\n",
        "        for model_entry in result_dict_no_opt:\n",
        "            model_type = model_entry['type']\n",
        "            target = model_entry['target']\n",
        "            if model_type not in type_to_target_to_entry:\n",
        "                type_to_target_to_entry[model_type] = {}\n",
        "            type_to_target_to_entry[model_type][target] = model_entry\n",
        "\n",
        "        # Now, for each model type, compute OEE_composite\n",
        "        for model_type, target_to_entry in type_to_target_to_entry.items():\n",
        "            # Check if we have 'PERF', 'AVAIL', 'QUAL', and 'OEE' in targets\n",
        "            required_targets = ['PERF', 'AVAIL', 'QUAL', 'OEE']\n",
        "            if all(t in target_to_entry for t in required_targets):\n",
        "                # Get the entries\n",
        "                perf_entry = target_to_entry['PERF']\n",
        "                avail_entry = target_to_entry['AVAIL']\n",
        "                qual_entry = target_to_entry['QUAL']\n",
        "                oee_entry = target_to_entry['OEE']\n",
        "\n",
        "                # Get the predictions and actual values\n",
        "                y_pred_perf = perf_entry['y_pred']\n",
        "                y_pred_avail = avail_entry['y_pred']\n",
        "                y_pred_qual = qual_entry['y_pred']\n",
        "                y_val_oee = oee_entry['y_val'].reset_index(drop=True)  # Actual OEE values\n",
        "\n",
        "                # Ensure that the predictions are aligned\n",
        "                # Reset indices to ensure alignment\n",
        "                y_pred_perf = pd.Series(y_pred_perf).reset_index(drop=True)\n",
        "                y_pred_avail = pd.Series(y_pred_avail).reset_index(drop=True)\n",
        "                y_pred_qual = pd.Series(y_pred_qual).reset_index(drop=True)\n",
        "\n",
        "                # Compute OEE_composite\n",
        "                oee_composite = y_pred_perf * y_pred_avail * y_pred_qual\n",
        "\n",
        "                # Compute metrics between oee_composite and y_val_oee\n",
        "                rmse, mae, r2 = calculate_metrics(y_val_oee, oee_composite)\n",
        "\n",
        "                # Store results\n",
        "                all_results.append(['OEE_composite', model_type, 'RMSE', rmse, None, None])\n",
        "                all_results.append(['OEE_composite', model_type, 'MAE', mae, None, None])\n",
        "                all_results.append(['OEE_composite', model_type, 'R2', r2, None, None])\n",
        "\n",
        "            else:\n",
        "                # Missing one or more required targets, skip this model type\n",
        "                print(f\"Model type '{model_type}' does not have all required targets. Skipping.\")\n",
        "    else:\n",
        "        # Both optimized and non-optimized models provided\n",
        "        # Build mappings for both\n",
        "        type_to_target_to_entry_no_opt = {}\n",
        "        for model_entry in result_dict_no_opt:\n",
        "            model_type = model_entry['type']\n",
        "            target = model_entry['target']\n",
        "            if model_type not in type_to_target_to_entry_no_opt:\n",
        "                type_to_target_to_entry_no_opt[model_type] = {}\n",
        "            type_to_entry = type_to_target_to_entry_no_opt[model_type]\n",
        "            type_to_entry[target] = model_entry\n",
        "\n",
        "        type_to_target_to_entry_opt = {}\n",
        "        for model_entry in result_dict_opt:\n",
        "            model_type = model_entry['type']\n",
        "            target = model_entry['target']\n",
        "            if model_type not in type_to_target_to_entry_opt:\n",
        "                type_to_target_to_entry_opt[model_type] = {}\n",
        "            type_to_entry = type_to_target_to_entry_opt[model_type]\n",
        "            type_to_entry[target] = model_entry\n",
        "\n",
        "        # For each model type, compute OEE_composite for both non-optimized and optimized models\n",
        "        for model_type in type_to_target_to_entry_no_opt.keys():\n",
        "            target_to_entry_no_opt = type_to_target_to_entry_no_opt[model_type]\n",
        "            target_to_entry_opt = type_to_target_to_entry_opt.get(model_type, {})\n",
        "\n",
        "            # Check if we have all required targets in both\n",
        "            required_targets = ['PERF', 'AVAIL', 'QUAL', 'OEE']\n",
        "            if all(t in target_to_entry_no_opt for t in required_targets) and all(t in target_to_entry_opt for t in required_targets):\n",
        "                # Get the entries for non-optimized models\n",
        "                perf_entry_no_opt = target_to_entry_no_opt['PERF']\n",
        "                avail_entry_no_opt = target_to_entry_no_opt['AVAIL']\n",
        "                qual_entry_no_opt = target_to_entry_no_opt['QUAL']\n",
        "                oee_entry_no_opt = target_to_entry_no_opt['OEE']\n",
        "\n",
        "                # Get the entries for optimized models\n",
        "                perf_entry_opt = target_to_entry_opt['PERF']\n",
        "                avail_entry_opt = target_to_entry_opt['AVAIL']\n",
        "                qual_entry_opt = target_to_entry_opt['QUAL']\n",
        "                oee_entry_opt = target_to_entry_opt['OEE']\n",
        "\n",
        "                # Get the predictions and actual values for non-optimized models\n",
        "                y_pred_perf_no_opt = pd.Series(perf_entry_no_opt['y_pred']).reset_index(drop=True)\n",
        "                y_pred_avail_no_opt = pd.Series(avail_entry_no_opt['y_pred']).reset_index(drop=True)\n",
        "                y_pred_qual_no_opt = pd.Series(qual_entry_no_opt['y_pred']).reset_index(drop=True)\n",
        "                y_val_oee_no_opt = oee_entry_no_opt['y_val'].reset_index(drop=True)\n",
        "\n",
        "                # Get the predictions and actual values for optimized models\n",
        "                y_pred_perf_opt = pd.Series(perf_entry_opt['y_pred']).reset_index(drop=True)\n",
        "                y_pred_avail_opt = pd.Series(avail_entry_opt['y_pred']).reset_index(drop=True)\n",
        "                y_pred_qual_opt = pd.Series(qual_entry_opt['y_pred']).reset_index(drop=True)\n",
        "                y_val_oee_opt = oee_entry_opt['y_val'].reset_index(drop=True)\n",
        "\n",
        "                # Compute OEE_composite for non-optimized models\n",
        "                oee_composite_no_opt = y_pred_perf_no_opt * y_pred_avail_no_opt * y_pred_qual_no_opt\n",
        "\n",
        "                # Compute OEE_composite for optimized models\n",
        "                oee_composite_opt = y_pred_perf_opt * y_pred_avail_opt * y_pred_qual_opt\n",
        "\n",
        "                # Compute metrics for non-optimized models\n",
        "                rmse_no_opt, mae_no_opt, r2_no_opt = calculate_metrics(y_val_oee_no_opt, oee_composite_no_opt)\n",
        "\n",
        "                # Compute metrics for optimized models\n",
        "                rmse_opt, mae_opt, r2_opt = calculate_metrics(y_val_oee_opt, oee_composite_opt)\n",
        "\n",
        "                # Calculate differences\n",
        "                rmse_diff = rmse_opt - rmse_no_opt\n",
        "                mae_diff = mae_opt - mae_no_opt\n",
        "                r2_diff = r2_opt - r2_no_opt\n",
        "\n",
        "                # Store results\n",
        "                all_results.append(['OEE_composite', model_type, 'RMSE', rmse_no_opt, rmse_opt, rmse_diff])\n",
        "                all_results.append(['OEE_composite', model_type, 'MAE', mae_no_opt, mae_opt, mae_diff])\n",
        "                all_results.append(['OEE_composite', model_type, 'R2', r2_no_opt, r2_opt, r2_diff])\n",
        "\n",
        "            else:\n",
        "                # Missing targets\n",
        "                print(f\"Model type '{model_type}' does not have all required targets in both optimized and non-optimized models. Skipping.\")\n",
        "    # Create DataFrame\n",
        "    df_results = pd.DataFrame(all_results, columns=['Target', 'Model Type', 'Metric', 'Non-Optimized', 'Optimized', 'Difference'])\n",
        "    df_pivot = df_results.pivot_table(index=['Target', 'Model Type', 'Metric'], values=['Non-Optimized', 'Optimized', 'Difference'], aggfunc='first')\n",
        "\n",
        "    return df_pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pivot = revalidate_models_all_targets(results_no_hpo, results_hpo)\n",
        "df_pivot = pd.concat([df_pivot, calculate_composite_metrics(results_no_hpo, results_hpo)])\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to calculate errors for each sample\n",
        "def calculate_sample_errors(result_model_performance):\n",
        "    errors = []\n",
        "\n",
        "    for entry in result_model_performance:\n",
        "        y_val = entry['y_val']\n",
        "        y_pred = entry['y_pred']\n",
        "        sample_errors = (np.array(y_val) - np.array(y_pred)).flatten()  # Flatten the array\n",
        "        errors.append({\n",
        "            'type': entry['type'],\n",
        "            'target': entry['target'],\n",
        "            'sample_errors': sample_errors\n",
        "        })\n",
        "    \n",
        "    return errors\n",
        "\n",
        "# Function to create a DataFrame of errors for a specific target\n",
        "def create_error_df(errors, target, models):\n",
        "    error_dict = {}\n",
        "\n",
        "    for entry in errors:\n",
        "        if entry['target'] == target and entry['type'] in models:\n",
        "            error_dict[entry['type']] = entry['sample_errors']\n",
        "    \n",
        "    # Filter out entries with empty lists\n",
        "    error_dict = {k: v for k, v in error_dict.items() if len(v) > 0}\n",
        "\n",
        "    # Check for consistent lengths\n",
        "    lengths = [len(v) for v in error_dict.values()]\n",
        "    if len(set(lengths)) != 1:\n",
        "        raise ValueError(\"Inconsistent sample sizes across models for the target: {}\".format(target))\n",
        "    \n",
        "    error_df = pd.DataFrame(error_dict)\n",
        "    return error_df\n",
        "\n",
        "# Function to compute and plot correlation of sample errors\n",
        "def analyze_sample_errors(errors, targets, models):\n",
        "    for target in targets:\n",
        "        try:\n",
        "            error_df = create_error_df(errors, target, models)\n",
        "            if not error_df.empty:\n",
        "                # Calculate and plot the correlation matrix\n",
        "                corr_matrix = error_df.corr()\n",
        "               # Adjust font size of the annotations\n",
        "                plt.figure(figsize=(12, 10), dpi=300)\n",
        "                sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.3f',\n",
        "                            xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns,\n",
        "                            annot_kws={\"size\": 12})  # Adjust the font size here\n",
        "\n",
        "                #plt.title(f'Sample Error Correlation Matrix for {target}', fontsize=16)\n",
        "                plt.show()\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping target {target}: {e}\")\n",
        "\n",
        "# Define targets and models\n",
        "targets = ['OEE'] # Possible / Tested = ['OEE', 'PERF', 'AVAIL', 'QUAL', 'DT', 'APT', 'PBT']\n",
        "models = ['linear', 'ridge', 'poly', 'dt', 'rf', 'svr', 'xgb', 'catboost', 'lgbm', 'NN'] # Possible / Tested = ['linear', 'ridge', 'poly', 'dt', 'rf', 'xgb', 'svr', 'catboost', 'lgbm', 'NN']\n",
        "\n",
        "# Calculate sample errors\n",
        "sample_errors = calculate_sample_errors(results_no_hpo)\n",
        "\n",
        "# Analyze and visualize sample errors\n",
        "analyze_sample_errors(sample_errors, targets, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explainability Charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "from catboost import Pool\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def explain_model_prediction(model_key, model, X_train, X_test_instance):\n",
        "    # Convert X_train and X_test_instance to catboost.Pool\n",
        "    train_pool = Pool(X_train)\n",
        "    test_pool = Pool(X_test_instance)\n",
        "\n",
        "    # Initialize the explainer based on the model type\n",
        "    if model_key == 'catboost':\n",
        "        # Pass the training data as Pool to calculate SHAP values\n",
        "        shap_values = model.get_feature_importance(data=train_pool, type='ShapValues')\n",
        "        shap_values = shap_values[:len(X_test_instance)]\n",
        "        shap_values = shap_values[:, :-1]  # Remove the last column (base value)\n",
        "    elif model_key == 'svr':\n",
        "        print(\"SHAP explanations for SVR or other non-tree models need a different approach.\")\n",
        "    else:\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "        shap_values = explainer.shap_values(X_test_instance)\n",
        "\n",
        "    # Plot the SHAP values for the test instance\n",
        "    shap.initjs()\n",
        "    if model_key in ['rf', 'xgb', 'lgbm', 'catboost']:\n",
        "        shap.summary_plot(shap_values, X_test_instance, feature_names=X_test_instance.columns, plot_type=\"bar\")\n",
        "        shap.summary_plot(shap_values, X_test_instance, feature_names=X_test_instance.columns)\n",
        "        shap.dependence_plot('OrderQuantity', shap_values, X_test_instance, interaction_index=None)\n",
        "        shap.dependence_plot('10th_Percentile_Auftragswechsel', shap_values, X_test_instance, interaction_index=None)\n",
        "        shap.dependence_plot('FS_Breite', shap_values, X_test_instance, interaction_index=None)\n",
        "    elif model_key == 'svr':\n",
        "        print(\"SHAP explanations for SVR or other non-tree models need a different approach.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for model in results_no_hpo:\n",
        "    if model['type'] == 'catboost' and model['target']=='OEE':\n",
        "        #print(model['X_val'].columns)\n",
        "        print(f\"Starting analysis for model {model['type']} for target {model['target']}\")\n",
        "        explain_model_prediction(model['type'], model['model'], pd.DataFrame(model['X_val']), pd.DataFrame(model['X_val']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scienceplots\n",
        "import pandas as pd\n",
        "\n",
        "# Use the 'science' style for plots\n",
        "plt.style.use('science')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a DataFrame\n",
        "model_effiency_analysis = pd.DataFrame(results)\n",
        "\n",
        "# Plotting function\n",
        "def plot_model_performance(dataframe, error_function):\n",
        "    # Get unique targets\n",
        "    targets = dataframe['target'].unique()\n",
        "    best_models = []\n",
        "\n",
        "    for target in targets:\n",
        "        # Filter data for the current target\n",
        "        target_data = dataframe[dataframe['target'] == target]\n",
        "        target_data = target_data.sort_values(by='type')\n",
        "\n",
        "        # Find the minimum error value\n",
        "        min_error = target_data[error_function].min()\n",
        "        min_model = target_data[target_data[error_function]==min_error]\n",
        "\n",
        "        best_models.append(\n",
        "            {\n",
        "                \"target\": target,\n",
        "                \"model\": min_model['model'],\n",
        "                \"name\": min_model['type'].values[0],\n",
        "                \"error\": min_error,\n",
        "                \"error_name\": error_function,\n",
        "                \"X_val\": min_model['X_val']\n",
        "            }\n",
        "        )\n",
        "        # Generate colors based on the RMSE value\n",
        "        colors = ['green' if rmse == min_error else 'skyblue' for rmse in target_data[error_function]]\n",
        "\n",
        "        # Create a bar plot for the current target\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        # Update the plt.bar line to use the colors list\n",
        "        plt.bar(target_data['type'], target_data[error_function], color=colors)\n",
        " \n",
        "        # Adding title and labels\n",
        "        plt.title(f'Model Performance for Target: {target}')\n",
        "        plt.xlabel('Model Type')\n",
        "        plt.ylabel(error_function)\n",
        "        plt.xticks(rotation=90)  # for vertical labels\n",
        " \n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "\n",
        "    return best_models\n",
        "\n",
        "# Call the function to plot the performance of models\n",
        "model_eval = plot_model_performance(model_effiency_analysis, 'val_RMSE')\n",
        "#print(\"~~~~~~~~~~~~~~~~~~~~~~~\\nMEAN AVERAGE ERROR\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "#model_eval = plot_model_performance(model_effiency_analysis, 'MAE')"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
